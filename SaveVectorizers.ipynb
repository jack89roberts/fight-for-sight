{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'eye_in_new_keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_abstracts(abstracts):\n",
    "\n",
    "    # remove missing/very short abstracts\n",
    "    print('Removing missing abstracts')\n",
    "    abstracts = abstracts.dropna()\n",
    "    abstracts = abstracts[abstracts.str.split(r'[^a-zA-Z]+').str.len()>10]\n",
    "    \n",
    "    # convert to lower case and remove digits and punctuation\n",
    "    print('Initial preprocessing: case, punctuation, whitespace')\n",
    "    abstracts = abstracts.str.lower()\n",
    "    abstracts = abstracts.str.replace(r'[^a-zA-Z]+',' ',regex=True)\n",
    "    abstracts = abstracts.str.strip()\n",
    "    abstracts= abstracts.str.split(' ')\n",
    "\n",
    "    print('Lemmatizing')\n",
    "    try:\n",
    "        lemma = WordNetLemmatizer()\n",
    "    except:\n",
    "        import nltk\n",
    "        nltk.download('wordnet')\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "    try:\n",
    "        stop_words = stopwords.words('english')\n",
    "    except:\n",
    "        import nltk\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "    abstracts=abstracts.apply(lambda x : [lemma.lemmatize(word) for word in x if word not in stop_words])\n",
    "    abstracts=abstracts.apply(lambda x : [word for word in x if len(word)>1])\n",
    "    abstracts=abstracts.apply(lambda x : ' '.join(x))\n",
    "\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409266 rows\n",
      "Removing missing abstracts\n",
      "Initial preprocessing: case, punctuation, whitespace\n",
      "Lemmatizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pmid\n",
       "30209082    ass prevalence cause vision impairment north a...\n",
       "29781739    developed country genetically inherited eye di...\n",
       "30092731    systemic autoimmune disease associated ocular ...\n",
       "30096011    ass prevalence ocular manifestation related di...\n",
       "30270476    optical coherence tomography oct provides non ...\n",
       "Name: abstractText, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data/EPMC/'+file_name+'.pkl')\n",
    "df.drop_duplicates(subset='pmid',inplace=True)\n",
    "df.set_index('pmid',inplace=True)\n",
    "print(len(df),'rows')\n",
    "df.head()\n",
    "\n",
    "abstracts = df.loc[~df['abstractText'].isnull(),'abstractText']\n",
    "\n",
    "del df\n",
    "\n",
    "abstracts = lemmatize_abstracts(abstracts)\n",
    "\n",
    "pmids = abstracts.index\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.to_pickle('data/EPMC/'+file_name+'_ABSTRACTS_LEMMA.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting countvec\n",
      "size vocab: 27509\n",
      "no. stop words: 10227181\n",
      "saving vectoriser\n",
      "transforming abstracts\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('fitting countvec')\n",
    "countvec = CountVectorizer(strip_accents='unicode', lowercase=True, stop_words='english', \n",
    "                           ngram_range=(1, 2), max_df=0.2, min_df=0.0005, max_features=None)\n",
    "\n",
    "# 'fit' the vectorizer to the corpus\n",
    "# this step automatically determines the vocabulary\n",
    "countvec.fit(abstracts)\n",
    "\n",
    "print('size vocab:',len(countvec.vocabulary_ ))\n",
    "print('no. stop words:',len(countvec.stop_words_))\n",
    "\n",
    "print('saving vectoriser')\n",
    "joblib.dump(countvec, 'data/CountVec.joblib') \n",
    "\n",
    "features = countvec.get_feature_names()\n",
    "\n",
    "# then 'transform' the corpus\n",
    "# this computes the term frequency vectors\n",
    "print('transforming abstracts')\n",
    "countvec_vectors = countvec.transform(abstracts)\n",
    "\n",
    "del countvec\n",
    "\n",
    "#print('saving transformed abstract vectors')\n",
    "#countvec_vectors = pd.DataFrame(countvec_vectors, index=pmids,columns=features)\n",
    "#countvec_vectors.to_pickle('data/EPMC/'+file_name+'_COUNT_VECTORS.pkl')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.sum(countvec_vectors>1,axis=0)\n",
    "df = np.squeeze(np.array(df))\n",
    "df = pd.Series(df,index=features)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = df/len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = countvec.vocabulary_\n",
    "idf = countvec.idf_\n",
    "pd.Series(idf).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ((1+len(abstracts))/np.exp(idf-1))-1\n",
    "df = pd.Series(df,index=features)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.sort_values(ascending=False).head(40)/len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = df/len(abstracts)\n",
    "df_frac.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins = pd.cut(df,[0,1.5,10.5,100.5,1000.5,10000.5,100000.5,1000000.5])\n",
    "df_bins.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index.str.len()<3].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum((df_frac>0.005) & (df_frac<0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
